{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "r_0MEXYvXWec",
   "metadata": {
    "id": "r_0MEXYvXWec"
   },
   "source": [
    "# Question 2: AI Community\n",
    "## Akshat Kumar\n",
    "## 22B4513"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f8ec9-e319-473f-86db-bfc49ac305aa",
   "metadata": {
    "id": "938f8ec9-e319-473f-86db-bfc49ac305aa"
   },
   "source": [
    "## Building RAG chatbots with LangChain and Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kSno0yNMWiyj",
   "metadata": {
    "id": "kSno0yNMWiyj"
   },
   "source": [
    "## Using Publicly Available LLMs to Enhance Markdown Explanations\n",
    "\n",
    "I utilized publicly available language models (LLMs) to help me write clear and effective markdown explanations for my code. Here's the process:\n",
    "\n",
    "### Prompt\n",
    "\"I write my own explanation of the code.\"\n",
    "\n",
    "### Logic of the Code\n",
    "- **Passing the Code**: The code is passed to the language model as input.\n",
    "- **Generating Explanations**: The language model processes the input code and generates a detailed and well-structured explanation in markdown format.\n",
    "- **Review and Refinement**: The generated explanation is reviewed and refined as needed to ensure clarity and accuracy.\n",
    "\n",
    "This approach leverages the capabilities of LLMs to improve the quality and readability of code documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d2c48-3314-44da-a12a-61841c345e65",
   "metadata": {
    "id": "162d2c48-3314-44da-a12a-61841c345e65"
   },
   "source": [
    "### install libraries\n",
    "```shell\n",
    "!pip install -U   \\\n",
    "    langchain     \\\n",
    "    openai        \\\n",
    "    datasets      \\\n",
    "    qdrant-client \\\n",
    "    tiktoken\n",
    "```\n",
    "\n",
    "I have seen that !pip install dotenv doesnt work well. So use this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9XBtqsFkSR8x",
   "metadata": {
    "id": "9XBtqsFkSR8x"
   },
   "outputs": [],
   "source": [
    "!python -m pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba2e2b-87f2-4820-952c-15ee62d1af93",
   "metadata": {
    "id": "69ba2e2b-87f2-4820-952c-15ee62d1af93",
    "outputId": "059ca3e1-5439-4aaf-d66c-05aeeb4072c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7Pv-tf0SocV",
   "metadata": {
    "id": "t7Pv-tf0SocV"
   },
   "source": [
    "### Enter Your OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa0b34-9182-498d-b63f-678203e99b02",
   "metadata": {
    "id": "a8aa0b34-9182-498d-b63f-678203e99b02"
   },
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo'\n",
    "    openai_api_key='your-openai-api-key' # Replace your-openai-api-key with you own API key to run the notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RRoupHhdTCJ6",
   "metadata": {
    "id": "RRoupHhdTCJ6"
   },
   "source": [
    "### Code Explanation\n",
    "\n",
    "The provided code snippet demonstrates how to create a series of messages between a human and an AI assistant using the `langchain.schema` module. Here’s a brief explanation of each part:\n",
    "\n",
    "1. **Importing Classes**:\n",
    "   - `SystemMessage`: Represents a message from the system, often used to set the context or initial instructions for the AI.\n",
    "   - `HumanMessage`: Represents a message from the human user.\n",
    "   - `AIMessage`: Represents a response from the AI.\n",
    "\n",
    "2. **Creating Messages**:\n",
    "   - The `messages` list contains a sequence of interactions, starting with a `SystemMessage` to set the context.\n",
    "   - The first `HumanMessage` is a greeting from the user: \"Hi AI, how are you today?\"\n",
    "   - The `AIMessage` provides a friendly response from the AI: \"I'm great thank you. How can I help you?\"\n",
    "   - The second `HumanMessage` indicates the user’s request: \"I'd like to understand machine learning.\"\n",
    "\n",
    "This sequence of messages sets up a conversational context where the AI assistant is positioned as helpful and ready to provide information about machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52c263-2b0e-4acd-95e7-e6a04056a0cf",
   "metadata": {
    "id": "8f52c263-2b0e-4acd-95e7-e6a04056a0cf"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand machine learning.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xT7dKTd3T8sl",
   "metadata": {
    "id": "xT7dKTd3T8sl"
   },
   "source": [
    "The following code snippet continues the conversation with the AI assistant and appends the AI's response to the message list:\n",
    "\n",
    "1. **Invoking the AI**:\n",
    "   - `res = chat.invoke(messages)`: Sends the current list of messages to the AI and stores the response in `res`.\n",
    "\n",
    "2. **Appending the Response**:\n",
    "   - `messages.append(res)`: Adds the AI's response to the `messages` list for further interactions.\n",
    "\n",
    "This allows for an ongoing conversation where each new message is appended to the existing list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dfe9d-c6d5-4e75-ab80-1e5e03c7f227",
   "metadata": {
    "id": "940dfe9d-c6d5-4e75-ab80-1e5e03c7f227",
    "outputId": "72406f40-0356-4c23-95f5-336b62591381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning is a subset of artificial intelligence that involves developing algorithms and statistical models that allow computers to learn from and make predictions or decisions based on data. The main goal of machine learning is to enable computers to automatically learn and improve from experience without being explicitly programmed. There are various types of machine learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, each serving different purposes and applications. Do you have any specific questions about machine learning that I can help clarify for you?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat.invoke(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91387c13-7707-48e4-ad62-a996b3b15df9",
   "metadata": {
    "id": "91387c13-7707-48e4-ad62-a996b3b15df9",
    "outputId": "a7899e82-4ac3-4743-a270-ddfe531b6b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of artificial intelligence that involves developing algorithms and statistical models that allow computers to learn from and make predictions or decisions based on data. The main goal of machine learning is to enable computers to automatically learn and improve from experience without being explicitly programmed. There are various types of machine learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, each serving different purposes and applications. Do you have any specific questions about machine learning that I can help clarify for you?\n"
     ]
    }
   ],
   "source": [
    "# Printing AI Response\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b647ef7-13b7-4056-ab3d-8477afcbbd96",
   "metadata": {
    "id": "2b647ef7-13b7-4056-ab3d-8477afcbbd96"
   },
   "outputs": [],
   "source": [
    "# Remember the new conversation\n",
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02202f41-a839-4871-ab72-128e3808da12",
   "metadata": {
    "id": "02202f41-a839-4871-ab72-128e3808da12"
   },
   "outputs": [],
   "source": [
    "# Giving New Prompt and appending it\n",
    "prompt = HumanMessage(\n",
    "    content=\"Whats the difference between supervised and unsupervised?\"\n",
    ")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967369c8-629f-4def-86bd-7eddee92e36d",
   "metadata": {
    "id": "967369c8-629f-4def-86bd-7eddee92e36d"
   },
   "outputs": [],
   "source": [
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685571e1-5b0e-4180-adf6-905df89504dd",
   "metadata": {
    "id": "685571e1-5b0e-4180-adf6-905df89504dd",
    "outputId": "6e4cf649-419e-492a-bf85-d59be734b7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised and unsupervised learning are two main types of machine learning approaches that involve different methods of training models and making predictions:\n",
      "\n",
      "1. Supervised learning: In supervised learning, the algorithm is trained on a labeled dataset, where each example in the dataset is associated with a target output or label. The goal is for the model to learn a mapping between the input data and the corresponding output labels. During training, the model adjusts its parameters based on the difference between its predictions and the true labels in order to minimize the prediction error. Once the model is trained, it can make predictions on new, unseen data by generalizing from the labeled training examples.\n",
      "\n",
      "Examples of supervised learning tasks include classification (predicting discrete class labels) and regression (predicting continuous values). Common algorithms used in supervised learning include linear regression, logistic regression, support vector machines, decision trees, and neural networks.\n",
      "\n",
      "2. Unsupervised learning: In unsupervised learning, the algorithm is trained on an unlabeled dataset, where there are no target output labels provided. The goal of unsupervised learning is to find patterns, structures, or relationships in the data without explicit guidance. The algorithm aims to learn the inherent structure of the data, such as grouping similar data points together or reducing the dimensionality of the data.\n",
      "\n",
      "Clustering and dimensionality reduction are common tasks in unsupervised learning. Clustering algorithms, such as K-means clustering and hierarchical clustering, group data points based on their similarities. Dimensionality reduction techniques, such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), aim to reduce the complexity of the data by capturing its essential features.\n",
      "\n",
      "In summary, supervised learning requires labeled data for training, while unsupervised learning works with unlabeled data to discover patterns or structures. Both approaches have their own strengths and are used in various applications depending on the nature of the data and the desired outcomes.\n"
     ]
    }
   ],
   "source": [
    "# Printing the AI response\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1837b-d8b4-4843-9826-e649217613c8",
   "metadata": {
    "id": "76d1837b-d8b4-4843-9826-e649217613c8"
   },
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"What is so special about Mistral 7B?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec00081-3894-48ff-ba5c-089e6ae51c4c",
   "metadata": {
    "id": "0ec00081-3894-48ff-ba5c-089e6ae51c4c",
    "outputId": "da6681c2-64c0-43d0-f6b4-af9ce52a54f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 7B is a high-performance computing system developed by Atos, a global leader in digital transformation. The Mistral 7B supercomputer is notable for its advanced capabilities and specifications that make it suitable for demanding computational tasks across various fields, such as scientific research, engineering simulations, weather forecasting, and AI applications. Some of the key features and highlights of Mistral 7B include:\n",
      "\n",
      "1. High computational power: Mistral 7B is equipped with a powerful combination of processors, memory, and storage systems that enable it to perform complex calculations and simulations at a rapid pace. Its high-performance architecture allows for efficient processing of large datasets and computation-intensive tasks.\n",
      "\n",
      "2. Scalability and flexibility: The Mistral 7B supercomputer is designed to be scalable, allowing users to expand its computational capacity as needed. It offers flexibility in terms of configuring and optimizing the system for specific workloads or applications, making it suitable for a wide range of use cases.\n",
      "\n",
      "3. Energy efficiency: Despite its high performance capabilities, Mistral 7B is engineered to be energy-efficient, helping to reduce overall power consumption and operational costs. This focus on energy efficiency is important for large-scale computing facilities that require significant power resources.\n",
      "\n",
      "4. Advanced cooling systems: To ensure optimal performance and prevent overheating, Mistral 7B is equipped with advanced cooling systems that help maintain the system's temperature within the desired range. Effective cooling mechanisms are essential for maximizing the lifespan and efficiency of high-performance computing systems.\n",
      "\n",
      "Overall, Mistral 7B stands out for its cutting-edge technology, computational power, scalability, energy efficiency, and advanced cooling systems, making it a top choice for organizations and research institutions looking to leverage high-performance computing for complex and data-intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RPvmQbCAVRSy",
   "metadata": {
    "id": "RPvmQbCAVRSy"
   },
   "source": [
    "## Testing the Chatbot with Uncommon Questions\n",
    "\n",
    "We can see the chatbot is doing quite well and is able to answer a lot of common questions on ML and Mistral 7B. Now, let's ask something which GPT-3.5 is not trained on and see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97078638-fb99-4868-9091-f3f7b31a03d7",
   "metadata": {
    "id": "97078638-fb99-4868-9091-f3f7b31a03d7"
   },
   "outputs": [],
   "source": [
    "# add latest response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you tell me about the LLMChain in LangChain?\"\n",
    ")\n",
    "# append to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GPT\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22c0d9-57ac-48b4-bb82-d19ee34acb4f",
   "metadata": {
    "id": "5e22c0d9-57ac-48b4-bb82-d19ee34acb4f",
    "outputId": "a15d9fb4-c090-4a3c-819d-244099e40ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I couldn't find specific information about an \"LLMChain\" in relation to LangChain. It's possible that the term or concept you are referring to is either new, specialized, or not widely recognized in the public domain. If you can provide more context or details about LLMChain or LangChain, I'll do my best to assist you further or provide information based on the details you provide.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JbjObwRYVycD",
   "metadata": {
    "id": "JbjObwRYVycD"
   },
   "source": [
    "## Adding LLMChain Information Using RAG\n",
    "\n",
    "As we can see, this chatbot doesn't have any information related to `LLMChain`. So, let's now add this information using Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "The following code snippet demonstrates how to enhance the chatbot's knowledge by providing it with specific information about `LLMChain` using RAG. The process involves augmenting the chatbot's prompt with additional context before querying the model.\n",
    "\n",
    "1. **Define the Information**:\n",
    "   - `llmchain_information`: A list of strings containing detailed information about `LLMChain`.\n",
    "   - This information covers what `LLMChain` is, its components, and the purpose of the LangChain framework.\n",
    "\n",
    "2. **Combine the Information**:\n",
    "   - `source_knowledge = \"\\n\".join(llmchain_information)`: Combines the list of information strings into a single string, separated by newline characters.\n",
    "\n",
    "3. **Create the Augmented Prompt**:\n",
    "   - `query`: The specific question we want to ask the chatbot.\n",
    "   - `augmented_prompt`: Combines the source knowledge with the query in a structured format. The prompt instructs the model to use the provided contexts to answer the question.\n",
    "\n",
    "4. **Create the HumanMessage**:\n",
    "   - `prompt = HumanMessage(content=augmented_prompt)`: Constructs a message with the augmented prompt, simulating a human query with additional context.\n",
    "\n",
    "5. **Append the Prompt and Invoke the Chatbot**:\n",
    "   - `messages.append(prompt)`: Adds the augmented prompt to the list of messages.\n",
    "   - `res = chat.invoke(messages)`: Sends the updated list of messages to the chatbot and stores the response in `res`.\n",
    "\n",
    "This approach uses RAG to improve the chatbot's ability to provide accurate and detailed information about topics it was not originally trained on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145b4cc-cfbf-4059-8f18-ae33bc2cbd71",
   "metadata": {
    "id": "5145b4cc-cfbf-4059-8f18-ae33bc2cbd71"
   },
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d64ad3-5df6-4daa-bbb0-4c2af38ed82f",
   "metadata": {
    "id": "e5d64ad3-5df6-4daa-bbb0-4c2af38ed82f"
   },
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below to answer the question.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a06b59-f182-4f94-a234-fa8e072023ec",
   "metadata": {
    "id": "16a06b59-f182-4f94-a234-fa8e072023ec"
   },
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312776f-b8fe-4cb1-a4e8-91c1a95e196b",
   "metadata": {
    "id": "7312776f-b8fe-4cb1-a4e8-91c1a95e196b",
    "outputId": "fe8ab7e5-09dc-4b88-e981-ffe9185095bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of LangChain, the LLMChain is a common type of chain that plays a crucial role in the framework for developing applications powered by language models. The LLMChain consists of three main components: a PromptTemplate, a model (which can be an LLM or a ChatModel), and an optional output parser. \n",
      "\n",
      "Here's how the LLMChain works within the LangChain framework:\n",
      "1. Input variables are passed to the LLMChain.\n",
      "2. The PromptTemplate formats these input variables into a prompt.\n",
      "3. The formatted prompt is then passed to the model (LLM or ChatModel) for processing.\n",
      "4. The model generates an output based on the input prompt.\n",
      "5. Optionally, the output parser can be used to further process and format the output from the language model into a final usable format.\n",
      "\n",
      "The LLMChain is a key component within LangChain that facilitates the interaction between input data, language models, and output processing. By leveraging the LLMChain within the LangChain framework, developers can build powerful and differentiated applications that not only utilize language models via APIs but also incorporate data awareness and agency for the language model to interact with its environment.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ThF4e-TIYuqp",
   "metadata": {
    "id": "ThF4e-TIYuqp"
   },
   "source": [
    "Now we can see that the chatbot is able to answer the information about Langchain even though initially it wasn't able to do so because of RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979f62e-96c9-4ad9-91cd-9f3d11db2ba2",
   "metadata": {
    "id": "7979f62e-96c9-4ad9-91cd-9f3d11db2ba2"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac41677-3a8e-4802-9ced-c24761889a94",
   "metadata": {
    "id": "2ac41677-3a8e-4802-9ced-c24761889a94",
    "outputId": "f5bb8d37-5054-4094-c255-e699a6b00ebf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mPb-PlzlbSVu",
   "metadata": {
    "id": "mPb-PlzlbSVu"
   },
   "source": [
    "## Loading RAG Dataset\n",
    "###We are going to use the The Mistral 7B paper as the RAG data [Link](https://http://arxiv.org/pdf/2310.06825)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac680c80-db7f-4925-b3d7-7147000724a4",
   "metadata": {
    "id": "ac680c80-db7f-4925-b3d7-7147000724a4",
    "outputId": "28f4a81b-37f0-4842-8cf8-a1d4118460b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e041dd5-11da-48f7-9325-bf728f32be3e",
   "metadata": {
    "id": "4e041dd5-11da-48f7-9325-bf728f32be3e",
    "outputId": "6892264b-d708-4e53-a339-4c0f1adf5ed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2310.06825',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src',\n",
       " 'id': '2310.06825',\n",
       " 'title': 'Mistral 7B',\n",
       " 'summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.',\n",
       " 'source': 'http://arxiv.org/pdf/2310.06825',\n",
       " 'authors': ['Albert Q. Jiang',\n",
       "  'Alexandre Sablayrolles',\n",
       "  'Arthur Mensch',\n",
       "  'Chris Bamford',\n",
       "  'Devendra Singh Chaplot',\n",
       "  'Diego de las Casas',\n",
       "  'Florian Bressand',\n",
       "  'Gianna Lengyel',\n",
       "  'Guillaume Lample',\n",
       "  'Lucile Saulnier',\n",
       "  'Lélio Renard Lavaud',\n",
       "  'Marie-Anne Lachaux',\n",
       "  'Pierre Stock',\n",
       "  'Teven Le Scao',\n",
       "  'Thibaut Lavril',\n",
       "  'Thomas Wang',\n",
       "  'Timothée Lacroix',\n",
       "  'William El Sayed'],\n",
       " 'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
       " 'comment': 'Models and code are available at\\n  https://mistral.ai/news/announcing-mistral-7b/',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20231010',\n",
       " 'updated': '20231010',\n",
       " 'references': [{'id': '1808.07036'},\n",
       "  {'id': '1809.02789'},\n",
       "  {'id': '1904.10509'},\n",
       "  {'id': '2302.13971'},\n",
       "  {'id': '2009.03300'},\n",
       "  {'id': '2305.13245'},\n",
       "  {'id': '1904.09728'},\n",
       "  {'id': '1803.05457'},\n",
       "  {'id': '2103.03874'},\n",
       "  {'id': '1905.07830'},\n",
       "  {'id': '2308.12950'},\n",
       "  {'id': '2210.09261'},\n",
       "  {'id': '2310.06825'},\n",
       "  {'id': '2307.09288'},\n",
       "  {'id': '2304.06364'},\n",
       "  {'id': '1905.10044'},\n",
       "  {'id': '2110.14168'},\n",
       "  {'id': '2108.07732'},\n",
       "  {'id': '2107.03374'},\n",
       "  {'id': '1811.00937'},\n",
       "  {'id': '2004.05150'},\n",
       "  {'id': '1705.03551'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3cbff-614f-4cf3-b547-1d92481c060e",
   "metadata": {
    "id": "3df3cbff-614f-4cf3-b547-1d92481c060e"
   },
   "outputs": [],
   "source": [
    "# Converting dataset to pandas\n",
    "data = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a78c8-5acc-4625-990b-2141cfbab652",
   "metadata": {
    "id": "fc2a78c8-5acc-4625-990b-2141cfbab652",
    "outputId": "6f5ddd34-0dfb-42e6-ecd7-f661272b46a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>0</td>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>1</td>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>2</td>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>3</td>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2310.06825</td>\n",
       "      <td>4</td>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>2310.06825</td>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-para...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "      <td>[Albert Q. Jiang, Alexandre Sablayrolles, Arth...</td>\n",
       "      <td>[cs.CL, cs.AI, cs.LG]</td>\n",
       "      <td>Models and code are available at\\n  https://mi...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20231010</td>\n",
       "      <td>20231010</td>\n",
       "      <td>[{'id': '1808.07036'}, {'id': '1809.02789'}, {...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi chunk-id                                              chunk  \\\n",
       "0  2310.06825        0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  2310.06825        1  automated benchmarks. Our models are released ...   \n",
       "2  2310.06825        2  GQA significantly accelerates the inference sp...   \n",
       "3  2310.06825        3  Mistral 7B takes a significant step in balanci...   \n",
       "4  2310.06825        4  parameters of the architecture are summarized ...   \n",
       "\n",
       "           id       title                                            summary  \\\n",
       "0  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "1  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "2  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "3  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "4  2310.06825  Mistral 7B  We introduce Mistral 7B v0.1, a 7-billion-para...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2310.06825   \n",
       "1  http://arxiv.org/pdf/2310.06825   \n",
       "2  http://arxiv.org/pdf/2310.06825   \n",
       "3  http://arxiv.org/pdf/2310.06825   \n",
       "4  http://arxiv.org/pdf/2310.06825   \n",
       "\n",
       "                                             authors             categories  \\\n",
       "0  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "1  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "2  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "3  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "4  [Albert Q. Jiang, Alexandre Sablayrolles, Arth...  [cs.CL, cs.AI, cs.LG]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  Models and code are available at\\n  https://mi...        None   \n",
       "1  Models and code are available at\\n  https://mi...        None   \n",
       "2  Models and code are available at\\n  https://mi...        None   \n",
       "3  Models and code are available at\\n  https://mi...        None   \n",
       "4  Models and code are available at\\n  https://mi...        None   \n",
       "\n",
       "  primary_category published   updated  \\\n",
       "0            cs.CL  20231010  20231010   \n",
       "1            cs.CL  20231010  20231010   \n",
       "2            cs.CL  20231010  20231010   \n",
       "3            cs.CL  20231010  20231010   \n",
       "4            cs.CL  20231010  20231010   \n",
       "\n",
       "                                          references  \n",
       "0  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "1  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "2  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "3  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  \n",
       "4  [{'id': '1808.07036'}, {'id': '1809.02789'}, {...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1699d-4010-4c54-b769-cba216878984",
   "metadata": {
    "id": "86d1699d-4010-4c54-b769-cba216878984",
    "outputId": "c8afdaa4-079f-48c1-d54d-fd473e6f540d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...   \n",
       "1  automated benchmarks. Our models are released ...   \n",
       "2  GQA significantly accelerates the inference sp...   \n",
       "3  Mistral 7B takes a significant step in balanci...   \n",
       "4  parameters of the architecture are summarized ...   \n",
       "\n",
       "                            source  \n",
       "0  http://arxiv.org/pdf/2310.06825  \n",
       "1  http://arxiv.org/pdf/2310.06825  \n",
       "2  http://arxiv.org/pdf/2310.06825  \n",
       "3  http://arxiv.org/pdf/2310.06825  \n",
       "4  http://arxiv.org/pdf/2310.06825  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data[['chunk', 'source']]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99f4d0-391c-41f1-95fa-840d34668a71",
   "metadata": {
    "id": "ce99f4d0-391c-41f1-95fa-840d34668a71"
   },
   "source": [
    "## RAG\n",
    "### We create an instance of the DataFrameLoader class called loader. We provide two arguments to initialize the loader:\n",
    "\n",
    "**docs:** This is assumed to be a DataFrame object containing the documents we want to load. Each row of the DataFrame represents a document.\n",
    "\n",
    "**page_content_column=\"chunk\":** This argument specifies the name of the column in the DataFrame that contains the textual content of each document. Here, \"chunk\" is the name of the column where the content is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c04511-834c-45ca-b6a6-223d2bfb66ff",
   "metadata": {
    "id": "02c04511-834c-45ca-b6a6-223d2bfb66ff"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column=\"chunk\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240c2f7-0339-471b-9e40-09882fc503aa",
   "metadata": {
    "id": "2240c2f7-0339-471b-9e40-09882fc503aa",
    "outputId": "3875716f-e862-45e0-f1eb-7e3e8229685b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', metadata={'source': 'http://arxiv.org/pdf/2310.06825'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325928d-285f-4781-8c18-c6dcaaed8907",
   "metadata": {
    "id": "2325928d-285f-4781-8c18-c6dcaaed8907",
    "outputId": "3b4e5288-0e67-41b4-f12b-300a7518e56e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bnf2jshfc7cl",
   "metadata": {
    "id": "Bnf2jshfc7cl"
   },
   "source": [
    "## Setting Up Qdrant Vector Store\n",
    "\n",
    "In this code snippet, we're configuring the Qdrant vector store for our documents with Langchain and integrating it with OpenAI's text embeddings.\n",
    "\n",
    "### Step 1: Importing Required Modules\n",
    "\n",
    "We start by importing the necessary modules:\n",
    "\n",
    "- `Qdrant` from `langchain_community.vectorstores` module\n",
    "- `OpenAIEmbeddings` from `langchain_openai` module\n",
    "\n",
    "### Step 2: Initializing OpenAI Embeddings\n",
    "\n",
    "Next, we initialize the OpenAI embeddings model. Here, we're using the \"text-embedding-3-small\" model.\n",
    "\n",
    "### Step 3: Retrieving Qdrant Configuration\n",
    "\n",
    "We retrieve the Qdrant URL and API key from the environment variables.\n",
    "\n",
    "### Step 4: Creating Qdrant Vector Store\n",
    "\n",
    "We then create an instance of the Qdrant vector store using the `Qdrant.from_documents` method:\n",
    "\n",
    "- `documents`: This represents the documents we want to store in Qdrant.\n",
    "- `embedding`: This specifies the embeddings model to be used for encoding the documents.\n",
    "- `url`: This is the URL endpoint for the Qdrant service.\n",
    "- `collection_name`: This is the name of the collection where the documents will be stored in Qdrant.\n",
    "- `api_key`: This is the API key required for accessing the Qdrant service.\n",
    "\n",
    "This code demonstrates how to set up and configure a Qdrant vector store for efficient storage and retrieval of documents using Langchain and OpenAI's text embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WvUiQRHkdSDV",
   "metadata": {
    "id": "WvUiQRHkdSDV"
   },
   "source": [
    "# NOTE: To run this block of code you must enter your QDRANT_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bc1df-26b9-489c-b121-147a7fc609af",
   "metadata": {
    "id": "ec4bc1df-26b9-489c-b121-147a7fc609af"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Replace with your own information\n",
    "\n",
    "url = os.getenv(\"QDRANT_URL\")\n",
    "api_key = os.getenv(\"QDRANT_KEY\")\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    collection_name=\"chatbot\",\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2nG5R9VodxmG",
   "metadata": {
    "id": "2nG5R9VodxmG"
   },
   "source": [
    "Setting up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600e3615-f2c7-4175-ad21-cdc7b1f69b5a",
   "metadata": {
    "id": "600e3615-f2c7-4175-ad21-cdc7b1f69b5a",
    "outputId": "6c93d82a-6678-4524-922c-59d5c88124d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'e4dfd725-2da6-4495-9ceb-bac70ec767f8', '_collection_name': 'chatbot'}),\n",
       " Document(page_content='GQA significantly accelerates the inference speed, and also reduces the memory requirement during\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\nMistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more', metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': '46bd5ce8-d3ea-4285-b5db-7a7bc3962db9', '_collection_name': 'chatbot'}),\n",
       " Document(page_content='automated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src\\nWebpage: https://mistral.ai/news/announcing-mistral-7b/\\n1 Introduction\\nIn the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\\nperformance often necessitates an escalation in model size. However, this scaling tends to increase\\ncomputational costs and inference latency, thereby raising barriers to deployment in practical,\\nreal-world scenarios. In this context, the search for balanced models delivering both high-level\\nperformance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\\na carefully designed language model can deliver high performance while maintaining an efficient\\ninference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\\nwithout sacrificing performance on non-code related benchmarks.\\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during', metadata={'source': 'http://arxiv.org/pdf/2310.06825', '_id': 'e08cbf7d-858f-4c96-8538-1ebf0804d81c', '_collection_name': 'chatbot'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Mistral 7B?\"\n",
    "qdrant.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Whe0URKd1Pe",
   "metadata": {
    "id": "5Whe0URKd1Pe"
   },
   "source": [
    "## Custom Prompt Generation Function\n",
    "\n",
    "This function, `custom_prompt(query: str)`, generates a custom prompt for answering a query using the Qdrant similarity search results.\n",
    "\n",
    "### Function Explanation\n",
    "\n",
    "The function takes a single argument `query`, which represents the query string for which we want to generate a prompt.\n",
    "\n",
    "### Step 1: Similarity Search\n",
    "\n",
    "The function performs a similarity search using Qdrant for the given query string. It retrieves the top 3 most similar documents based on the query.\n",
    "\n",
    "### Step 2: Extracting Source Knowledge\n",
    "\n",
    "Next, the function extracts the textual content of the top 3 similar documents to use as the source knowledge for answering the query.\n",
    "\n",
    "### Step 3: Generating Prompt\n",
    "\n",
    "The function constructs a prompt by combining the source knowledge with the query string.\n",
    "\n",
    "### Step 4: Returning Prompt\n",
    "\n",
    "Finally, the function returns the generated prompt.\n",
    "\n",
    "This function facilitates the generation of custom prompts for answering queries based on the similarity search results from Qdrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabeb994-35c4-421a-8f76-559826ec87d7",
   "metadata": {
    "id": "eabeb994-35c4-421a-8f76-559826ec87d7"
   },
   "outputs": [],
   "source": [
    "def custom_prompt(query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\"Using the contexts below, answer the query:\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9529893-611d-4db8-8f30-8117ecd897ca",
   "metadata": {
    "id": "f9529893-611d-4db8-8f30-8117ecd897ca",
    "outputId": "775421db-7725-438f-a830-be06efdb770a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query:\n",
      "\n",
      "    Contexts:\n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
      "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
      "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
      "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
      "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
      "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
      "large language models efficient. Through our work, our aim is to help the community create more\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "1 Introduction\n",
      "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
      "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
      "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
      "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
      "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
      "a carefully designed language model can deliver high performance while maintaining an efficient\n",
      "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
      "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
      "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
      "without sacrificing performance on non-code related benchmarks.\n",
      "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "\n",
      "    Query: What is so special about Mistral 7B?\n"
     ]
    }
   ],
   "source": [
    "print(custom_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58d584-eed7-40b4-a762-aad0f4ce2cd7",
   "metadata": {
    "id": "0b58d584-eed7-40b4-a762-aad0f4ce2cd7",
    "outputId": "4bb60cd9-3386-4438-a4d3-1c186ed62f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 7B is a high-performance language model engineered by the Mistral AI team to achieve superior performance and efficiency. Here are some key features that make Mistral 7B stand out:\n",
      "\n",
      "1. **Model Size**: Mistral 7B is a 7-billion-parameter language model, making it one of the largest language models in existence. Its size allows for capturing complex patterns and relationships in language data, leading to better performance in various natural language processing tasks.\n",
      "\n",
      "2. **Performance**: Mistral 7B outperforms other large language models, such as the 13-billion-parameter Llama 2 model and the 34-billion-parameter LLaMa 34B model, across multiple benchmarks. It excels in tasks related to reasoning, mathematics, code generation, and following instructions.\n",
      "\n",
      "3. **Efficiency**: Mistral 7B is designed to balance high performance with efficiency. It leverages innovative attention mechanisms, such as grouped-query attention (GQA) and sliding window attention (SWA), to accelerate inference speed, reduce memory requirements, and handle longer sequences more effectively. This efficiency is crucial for real-time applications and deployment on cloud platforms.\n",
      "\n",
      "4. **Ease of Deployment and Fine-Tuning**: Mistral 7B is released under the Apache 2.0 license, making it accessible for easy deployment on various platforms. It comes with a reference implementation that simplifies deployment on cloud services like AWS, GCP, and Azure. Additionally, Mistral 7B is crafted for ease of fine-tuning across a wide range of tasks, enabling users to adapt the model to specific applications.\n",
      "\n",
      "5. **Community Contribution**: The Mistral AI team aims to help the community create more automated benchmarks by releasing Mistral 7B under an open-source license. By providing a high-performance and efficient language model, Mistral 7B contributes to advancements in natural language processing research and applications.\n",
      "\n",
      "Overall, Mistral 7B's combination of model size, performance, efficiency, ease of deployment, and community contribution makes it a special and impactful addition to the field of language modeling and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat.invoke(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UzzNoxzoePnt",
   "metadata": {
    "id": "UzzNoxzoePnt"
   },
   "source": [
    "After integrating RAG, we presented the Mistral 7B paper and then posed a query, allowing us to observe the improved performance of our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XhPwxNk3hR9m",
   "metadata": {
    "id": "XhPwxNk3hR9m"
   },
   "source": [
    "## Summary\n",
    "\n",
    "First, we utilized a simple GPT-3.5 model to create a basic chatbot. We asked various questions related to machine learning (ML), which it could answer easily and accurately. However, when queried about Mistral 7B, its response was somewhat satisfactory but not entirely accurate. Furthermore, it failed when asked about Langchain.\n",
    "\n",
    "Now, we've developed a new chatbot from scratch using Retrieval-Augmented Generation (RAG). Our implementation utilizes OpenAI's GPT-3.5-turbo LLM, integrated through LangChain's ChatOpenAI class. Additionally, we employed OpenAI's text-embedding-3-small for embedding and the Qdrant vector database as our knowledge base.\n",
    "\n",
    "With our new model, we observed significant improvements. It accurately answered questions related to Langchain and Mistral 7B, enhancing its capabilities beyond its original training data. This demonstrates the enhanced performance and versatility of our model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
